<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Øystein Sørensen" />

<meta name="date" content="2019-02-01" />

<title>BayesMallows: An R Package for Probabilistic Preference Learning with the Mallows Rank Model</title>






<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>

</head>

<body>




<h1 class="title toc-ignore"><code>BayesMallows</code>: An R Package for Probabilistic Preference Learning with the Mallows Rank Model</h1>
<h4 class="author"><em>Øystein Sørensen</em></h4>
<h4 class="date"><em>2019-02-01</em></h4>



<p>The <code>BayesMallows</code> package implements methods for Bayesian preference learning with the Mallows rank model, as originally described in <span class="citation">Vitelli et al. (2018)</span>, and further developed in <span class="citation">Asfaw et al. (2016)</span> and <span class="citation">Crispino et al. (2018)</span>. This vignette describes the usage of the package, starting from the complete data cases, through top-<span class="math inline">\(k\)</span> rankings, pairwise comparisons, and finally clustering. We refer to the above mentioned papers, as well as the review <span class="citation">Liu et al. (2018)</span> for a thorough description of the methods. The necessary methods for data preprocessing, tuning of algorithms, and assessment of the posterior distributions will be described along the way.</p>
<div id="overview-of-package" class="section level1">
<h1>Overview of Package</h1>
<div id="functions" class="section level2">
<h2>Functions</h2>
<p>Here is an overview of the most used functions. You can read their documentation and see examples with <code>?function_name</code>.</p>
<table>
<caption>Main functions in the <code>BayesMallows</code> package.</caption>
<colgroup>
<col width="35%"></col>
<col width="64%"></col>
</colgroup>
<thead>
<tr class="header">
<th align="left">Function Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>compute_mallows</code></td>
<td align="left">Compute the posterior distribution of the Bayesian Mallows model. This is the main function of the package. Returns an object of class <code>BayesMallows</code>.</td>
</tr>
<tr class="even">
<td align="left"><code>compute_mallows_mixtures</code></td>
<td align="left">Compute multiple Mallows models with different number of mixture components. This is a convenience function for determining the number of mixtures to use.</td>
</tr>
<tr class="odd">
<td align="left"><code>sample_mallows</code></td>
<td align="left">Sample from the Mallows model.</td>
</tr>
<tr class="even">
<td align="left"><code>plot.BayesMallows</code></td>
<td align="left">Quick plots of the posterior densities of parameters of the Mallows model.</td>
</tr>
<tr class="odd">
<td align="left"><code>assess_convergence</code></td>
<td align="left">Study the convergence of the Markov chain, in order to determine burnin and other algorithm parameters.</td>
</tr>
<tr class="even">
<td align="left"><code>plot_elbow</code></td>
<td align="left">Create an elbow plot for comparing models with different number of clusters.</td>
</tr>
<tr class="odd">
<td align="left"><code>plot_top_k</code></td>
<td align="left">Plot the top-<span class="math inline">\(k\)</span> rankings. Particularly relevant when the data is in the form of pairwise comparisons.</td>
</tr>
<tr class="even">
<td align="left"><code>assign_cluster</code></td>
<td align="left">Compute the cluster assignment of assessors.</td>
</tr>
<tr class="odd">
<td align="left"><code>compute_consensus</code></td>
<td align="left">Compute the CP or MAP consensus ranking of the latent ranks.</td>
</tr>
<tr class="even">
<td align="left"><code>compute_posterior_intervals</code></td>
<td align="left">Compute Bayesian posterior intervals for the parameters.</td>
</tr>
<tr class="odd">
<td align="left"><code>generate_initial_ranking</code></td>
<td align="left">Generate an initial ranking, for the case of missing data or pairwise comparisons.</td>
</tr>
<tr class="even">
<td align="left"><code>generate_transitive_closure</code></td>
<td align="left">Generate the transitive closure for a set of pairwise comparisons.</td>
</tr>
<tr class="odd">
<td align="left"><code>estimate_partition_function</code></td>
<td align="left">Estimate the partition function of the Mallows model using either importance sampling or an asymptotic approximation.</td>
</tr>
</tbody>
</table>
</div>
<div id="datasets" class="section level2">
<h2>Datasets</h2>
<p>Here is an overview of the example datasets in <code>BayesMallows</code>. You can read their documentation with <code>?dataset_name</code>, or search for an example in this vignette.</p>
<table>
<caption>Example datasets in the <code>BayesMallows</code> package.</caption>
<colgroup>
<col width="35%"></col>
<col width="64%"></col>
</colgroup>
<thead>
<tr class="header">
<th align="left">Dataset Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>beach_preferences</code></td>
<td align="left">Stated pairwise preferences between random subsets of 15 images of beaches, by 60 assessors.</td>
</tr>
<tr class="even">
<td align="left"><code>sushi_rankings</code></td>
<td align="left">Complete rankings of 10 types of sushi by 5000 assessors.</td>
</tr>
<tr class="odd">
<td align="left"><code>potato_visual</code></td>
<td align="left">Complete rankings of 20 potatoes by weight, based on visual inspection, by 12 assessors.</td>
</tr>
<tr class="even">
<td align="left"><code>potato_weighing</code></td>
<td align="left">Complete rankings of 20 potatoes by weight, where the assessors were allowed to weigh the potatoes in their hands, by 12 assessors.</td>
</tr>
<tr class="odd">
<td align="left"><code>potato_true_ranking</code></td>
<td align="left">Vector of true weight rankings for the 20 potatoes in the example datasets <code>potato_visual</code> and <code>potato_weighing</code>.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="mallows-rank-model" class="section level1">
<h1>Mallows’ Rank Model</h1>
<p>We here give an informal review of the Mallows model (<span class="citation">Mallows (1957)</span>). The distribution of a ranking <span class="math inline">\(r \in \mathcal{P}_{n}\)</span> of <span class="math inline">\(n\)</span> items is modeled as</p>
<p><span class="math display">\[ P(r | \alpha, \rho) = Z_{n}(\alpha)^{-1} \exp\left\{-\frac{\alpha}{n} d(r, \rho)\right\} 1_{\mathcal{P}_{n}}(r), \]</span></p>
<p>where <span class="math inline">\(\mathcal{P}_{n}\)</span> is the set of all permutations of <span class="math inline">\(1, \dots, n\)</span>, <span class="math inline">\(\alpha\)</span> is a scale parameter, <span class="math inline">\(\rho \in \mathcal{P}_{n}\)</span> is a latent consensus ranking, <span class="math inline">\(d(\cdot, \cdot)\)</span> is a distance measure, <span class="math inline">\(1_{S}(\cdot)\)</span> is the indicator function for the set <span class="math inline">\(S\)</span>, and <span class="math inline">\(Z_{n}(\alpha)\)</span> is the partition function, or normalizing constant.</p>
<p>Given <span class="math inline">\(N\)</span> observed rankings, <span class="math inline">\(R_{1}, \dots, R_{N}\)</span>, the likelihood of the model is</p>
<p><span class="math display">\[ P(R_{1}, \dots, R_{N} | \alpha, \rho) = Z_{n}(\alpha)^{-N} \exp\left\{-\frac{\alpha}{n} \sum_{j=1}^{N}d(R_{j}, \rho)\right\} \prod_{j=1}^{N} \left\{1_{\mathcal{P}_{n}}(R_{j}) \right\}. \]</span></p>
<p>The <code>rankings</code> argument to <code>compute_mallows</code> is assumed to be a matrix of the form <span class="math inline">\((R_{1}, R_{2}, \dots, R_{N})^{T}\)</span>, i.e., each row contains a ranking and each column is an item.</p>
<div id="prior-distributions" class="section level2">
<h2>Prior Distributions</h2>
<p>For <span class="math inline">\(\alpha\)</span> we use an exponential prior, with density <span class="math inline">\(\pi(\alpha | \lambda) = \lambda \exp(-\lambda \alpha).\)</span> The rate parameter <span class="math inline">\(\lambda\)</span> can be set by the user with the <code>lambda</code> argument to <code>compute_mallows</code>. For <span class="math inline">\(\rho\)</span> we assume a uniform prior distribution on <span class="math inline">\(\mathcal{P}_{n}\)</span>, with density <span class="math inline">\(\pi(\rho) = 1_{\mathcal{P}_{n}}(\rho) /n!.\)</span></p>
</div>
<div id="metropolis-hastings-algorithm" class="section level2">
<h2>Metropolis-Hastings Algorithm</h2>
<p>We use a Metropolis-Hastings algorithm for computing the posterior distributions of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\rho\)</span>. We propose <span class="math inline">\(\alpha\)</span> from a lognormal distribution <span class="math inline">\(\log \mathcal{N}(\log(\alpha), \sigma_{\alpha}^{2})\)</span>. We propose <span class="math inline">\(\rho\)</span> with a leap-and-shift algorithm, described in detail in <span class="citation">Vitelli et al. (2018)</span>. The standard deviation <span class="math inline">\(\sigma_{\alpha}^{2}\)</span> and the leap size can be set by the user with the arguments <code>alpha_prop_sd</code> and <code>leap_size</code> to <code>compute_mallows</code>.</p>
</div>
<div id="partial-rankings" class="section level2">
<h2>Partial Rankings</h2>
<p>If each assessor <span class="math inline">\(j\)</span> has ranked a subset of the items <span class="math inline">\(\mathcal{A}_{j}\)</span>, we use data augmentation to fill in the missing ranks. We define the augmented data vectors <span class="math inline">\(\tilde{R}_{1}, \dots, \tilde{R}_{N}\)</span>, and use a uniform prior for each assessor with support <span class="math inline">\(\mathcal(P)_{n} \setminus R_{j}\)</span>, i.e., the set of rankings not already chosen. The Metropolis-Hastings algorithm now alternates between sampling <span class="math inline">\(\tilde{R}_{1}, \dots, \tilde{R}_{N}\)</span> given the current <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\rho\)</span>, and sampling <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\rho\)</span> given the current <span class="math inline">\(\tilde{R}_{1}, \dots, \tilde{R}_{N}\)</span>.</p>
</div>
<div id="pairwise-comparisons" class="section level2">
<h2>Pairwise Comparisons</h2>
<p>When the assessors have stated a set of pairwise comparisons, rather than rankings, we use the same data augmentation ideas as for partial rankings, but the proposal distribution is slightly more complicated in order to ensure that the proposed ranking is compliant with the ordering implied by the pairwise comparisons. In addition, the transitive closure of the stated ordering has to be computed, in order to find all <em>implied orderings</em>. From a user perspective, no new algorithm parameters need to be considered.</p>
</div>
<div id="mixtures-of-mallows-models" class="section level2">
<h2>Mixtures of Mallows Models</h2>
<p>When the assessor pool is heterogeneous, one might assume that there exist several latent consensus rankings, <span class="math inline">\(\rho_{1}, \dots, \rho_{C}\)</span>, one for each cluster of assessors. Letting <span class="math inline">\(z_{1}, \dots, z_{N} \in\{1, \dots, C\}\)</span> assign each assessor to each cluster, the likelihood of the observed rankings is</p>
<p><span class="math display">\[P(R_{1}, \dots, R_{N} | \{\alpha_{c}, \rho_{c}\}_{c=1,\dots,C}, z_{1},\dots,z_{N}) = \prod_{j=1}^{N}\frac{1_{\mathcal{P}_{n}}(\rho)}{Z_{n}(\alpha_{z_{j}})}\exp\left\{ -\frac{\alpha_{z_{j}}}{n} d(R_{j}, \rho_{z_{j}})\right\}. \]</span> For the scale parameters <span class="math inline">\(\alpha_{1}, \dots, \alpha_{C}\)</span> we assume the exponential prior as before, all with the same rate parameter <span class="math inline">\(\lambda\)</span>. We assume that the cluster labels are a priori distributed according to <span class="math inline">\(P(z_{1}, \dots, z_{N} | \tau_{1}, \dots, \tau_{C}) = \prod_{j=1}^{N} \tau_{z_{j}}\)</span>, where <span class="math inline">\(\tau_{c}\)</span> is the a priori probability that an assessor belongs to cluster <span class="math inline">\(c\)</span>. For <span class="math inline">\(\tau_{1}, \dots, \tau_{C}\)</span> we assume the Dirichlet prior <span class="math inline">\(\pi(1, \dots, C) = \Gamma(\psi C)\Gamma(\psi)^{-C}\prod_{c=1}^{C}\tau_{c}^{\psi - 1}\)</span>, where <span class="math inline">\(\Gamma(\cdot)\)</span> is the gamma function. The user can control the value of <span class="math inline">\(\psi\)</span> with the <code>psi</code> argument to <code>compute_mallows</code>.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-asfaw2016">
<p>Asfaw, D., V. Vitelli, O. Sorensen, E. Arjas, and A. Frigessi. 2016. “Time‐varying Rankings with the Bayesian Mallows Model.” <em>Stat</em> 6 (1): 14–30. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.132">https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.132</a>.</p>
</div>
<div id="ref-crispino2018">
<p>Crispino, M., E. Arjas, V. Vitelli, N. Barrett, and A. Frigessi. 2018. “A Bayesian Mallows approach to non-transitive pair comparison data: how human are sounds?” <em>Accepted for Publication in Annals of Applied Statistics</em>.</p>
</div>
<div id="ref-liu2018">
<p>Liu, Q., M. Crispino, I. Scheel, V. Vitelli, and A. Frigessi. 2018. “Model-based learning from preference data.” <em>Manuscript</em>.</p>
</div>
<div id="ref-mallows1957">
<p>Mallows, C. L. 1957. “Non-Null Ranking Models. I.” <em>Biometrika</em> 44 (1/2): 114–30.</p>
</div>
<div id="ref-vitelli2018">
<p>Vitelli, V., O. Sorensen, M. Crispino, E. Arjas, and A. Frigessi. 2018. “Probabilistic Preference Learning with the Mallows Rank Model.” <em>Journal of Machine Learning Research</em> 18 (1): 1–49. <a href="http://jmlr.org/papers/v18/15-481.html">http://jmlr.org/papers/v18/15-481.html</a>.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
